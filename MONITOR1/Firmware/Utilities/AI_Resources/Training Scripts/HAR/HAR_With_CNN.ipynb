{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8 \n",
    "\n",
    "#   This software component is licensed by ST under BSD 3-Clause license,\n",
    "#   the \"License\"; You may not use this file except in compliance with the\n",
    "#   License. You may obtain a copy of the License at:\n",
    "#                        https://opensource.org/licenses/BSD-3-Clause\n",
    "  \n",
    "\n",
    "'''\n",
    "Training script of human activity recognition system (HAR), based on two different Convolutional Neural Network (CNN) architectures \n",
    "'''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step HAR Training using Artificial Networks and STM32CubeAI\n",
    "This notebook provides a step by step demonstration of a simple <u>H</u>uman <u>A</u>ctivity <u>R</u>ecognition system (HAR), based on a <u>C</u>onvolutional <u>N</u>eural <u>N</u>etworks (CNN). This Notebook uses a simple data preperation script through `DataHelper` class and let user to preprocess, split, and segment the dataset to bring it into the form which can be used for training and validation of the HAR CNN. For CNNs it uses a `CNNHandler` class which builds, trains and validates a CNN for a given set of input and output tensors. The `CNNHandler` comes with possibility to use one of the two provided example CNN architectures namely, **IGN** and **GMP**.\n",
    "\n",
    "All the implementations are done in [Python](https://www.python.org/) using [Tensorflow](https://www.tensorflow.org/).\n",
    "\n",
    "For demonstration purposes this script uses two datasets created for HAR using accelerometer sensor. \n",
    "\n",
    "* A public dataset provided by <u>WI</u>reless <u>S</u>ensing <u>D</u>ata <u>M</u>ining group named as **<u>WISDM</u>**. The details of the dataset are available [here](http://www.cis.fordham.edu/wisdm/dataset.php).\n",
    "\n",
    "* Our own propritery dataset called **<u>AST</u>**. \n",
    "\n",
    "**Note**: We are not providing any dataset in the function pack. The user can download WISDM dataset from [here](http://www.cis.fordham.edu/wisdm/dataset.php), while **<u>AST</u>** is a private dataset and is not provided. Although a subset of the **<u>AST</u>** dataset is provided in the function pack at location `/FP-AI-MONITOR1/Utilities/AI_Resources/Datasets/AST/`.\n",
    "\n",
    "Following figure shows the detailed workflow of CNN based HAR.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"760\" height=\"400\" src=\"workflow_nn.png\">    \n",
    "</p>\n",
    "\n",
    "Now, let us implement it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 : Import necessary dependencies\n",
    "Following section imports all the required dependencies. This also sets seeds for random number generators in Numpy and Tensorflow environments to make the results more deterministic between different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, tensorflow as tf, os, logging, warnings\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "\n",
    "# private libraries\n",
    "from PrepareDataset import DataHelper\n",
    "from HARNN import ANNModelHandler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for using callbacks to save the model during training and comparing the results at every epoch\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# disabling annoying warnings originating from Tensorflow\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "# disabling annoying warnings originating from python\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# setting the seeds to the random generators of Numpy and Tensorflow\n",
    "np.random.seed( 611 )\n",
    "tf.random.set_seed( 611 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Set environment variables\n",
    "Following section sets some user variables which will later be used for:\n",
    "\n",
    "* preparing the dataset,\n",
    "* building the neural networks,\n",
    "* training the neural networks, and\n",
    "* validating the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data variables\n",
    "dataset = 'AST' # or 'WISDM'\n",
    "reducedClasses = True # or False\n",
    "'''\n",
    "reducedClasses = True \n",
    "    For WISDM = > \n",
    "        will merge 'Sitting' and 'Standing' classes to Stationary, and \n",
    "        'Upstairs' and 'Downstairs' to 'Stairs'. \n",
    "    For AST = >\n",
    "        will remove 'Driving' Class '''\n",
    "segmentLength = 24\n",
    "stepSize = 24 # to control the overlap\n",
    "'''\n",
    "stepSize == segmentLength : No overlap\n",
    "stepSize < segmentlenght : Overlap of (segmentLength - stepSize) sampels\n",
    "stepSize > segmentlenght : Dropping ( stepSize - segmentLength ) sampels between adjacent samples\n",
    "'''\n",
    "preprocessing = True # or False. Rotate the samples so that the gravity points to 'z-axis' and supresses the gravity\n",
    "\n",
    "# neural network architecture variables\n",
    "modelName = 'IGN' # or 'GMP'\n",
    "\n",
    "# training variables\n",
    "trainTestSplit = 0.6\n",
    "trainValidationSplit = 0.7\n",
    "nEpochs = 20\n",
    "learningRate = 0.0005\n",
    "decay = 1e-6\n",
    "batchSize = 64\n",
    "verbosity = 1\n",
    "nrSamplesPostValid = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Result directory\n",
    "Each run can have different variables depending on user settings and to compare the results of different choices, such as different segment size for the window for data, different overlap settings etc. We can save the results to compare different runs. \n",
    "\n",
    "Following section creates a result directory to save results for the current run. The name of the directory has following format. `Mmm_dd_yyyy_hh_mm_ss`, and example name for directory can be `Jul_20_2021_14_31_20/`. In every result directory there will a `.txt` file with information on the run and a saved `.h5` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already exist create a parent directory for results.\n",
    "allRunsDir = './results_cnn/'\n",
    "if not os.path.exists( allRunsDir ):\n",
    "    os.mkdir( allRunsDir )\n",
    "thisRunDir = '{}/{}/'.format( allRunsDir, datetime.now().strftime( \"%Y_%b_%d_%H_%M_%S\" ) )\n",
    "os.mkdir( thisRunDir )\n",
    "infoString = 'runTime : {}\\nDatabase : {}\\nNetwork : {}\\nSeqLength : {}\\nStepSize : {}\\nEpochs : {}\\n'.format( datetime.now().strftime(\"%Y-%b-%d at %H:%M:%S\"), dataset, modelName, segmentLength, stepSize, nEpochs )\n",
    "with open( thisRunDir + 'info.txt', 'w' ) as text_file:\n",
    "    text_file.write( infoString )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Create a object of class `DataHelper` named `myDataHelper`\n",
    "The script in the following section creates a `DataHelper` object to preprocess, segment and split the dataset as well as to create one-hot-code labeling for the outputs to make the data training and testing ready using the choices set by the user in **Step2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataHelper = DataHelper( dataset = dataset, reducedClasses = reducedClasses, seqLength = segmentLength, \n",
    "                          seqStep = stepSize, preprocessing = preprocessing, trainTestSplit = trainTestSplit,\n",
    "                          trainValidSplit = trainValidationSplit, resultDir = thisRunDir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Prepare the dataset\n",
    "Following section prepares the dataset and create six tensors namely `TrainX`, `TrainY`, `ValidationX`, `ValidationY`, `TestX`, `TestY`. Each of the variables with trailing `X` are the inputs with shape `[_, segmentLength, nr_of_axes = 3, nr_channels = 1 ]`and each of the variables with trailing `Y` are corresponding outputs with shape `[ _, NrClasses ]`. `NrClasses` for `WISDM` can be `4` or `6` and for `AST` are `4` or `5` depending if the `reducedClasses` flag is set to `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainX, TrainY, ValidationX, ValidationY, TestX, TestY = myDataHelper.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print the number of samples in train, test and validation data sets and the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Number of training samples : {}\\nNumber of validation samples : {}\\nNumber of test samples : {}\\nNumber of classes : {}'.\\\n",
    "      format( TrainX.shape[0], ValidationX.shape[0], TestX.shape[0], TrainY.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6: Create an object of classe `ANNModelHandler`\n",
    "The script in the following section creates a `ANNModelHandler` object to create, train and validate the <u>C</u>onvolutional <u>N</u>eural <u>N</u>etwork (**CNN**) using the variables created in **Step2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myHarHandler = ANNModelHandler( modelName = modelName, classes = myDataHelper.classes, resultDir = thisRunDir,\n",
    "                              inputShape = TrainX.shape, outputShape = TrainY.shape, learningRate = learningRate,\n",
    "                              decayRate = decay, nEpochs = nEpochs, batchSize = batchSize,\n",
    "                              modelFileName = 'har_' + modelName, verbosity = verbosity )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6.1: Create a CNN model\n",
    "Following script creates the **<u>CNN</u>** and prints its summary to show the architecture and provide the information on the trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harModel = myHarHandler.build_model()\n",
    "harModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6.2: Create a Checkpoint for ANN training\n",
    "The following script creates a check point for the training process of ANN to save the neural network as `h5` file. The settings are used in a way that the copy with maximum validation accuracy `val_acc` is saved only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harModelCheckPoint = tf.keras.callbacks.ModelCheckpoint( filepath = join( thisRunDir, 'har_' + modelName + '.h5' ),\n",
    "                                     monitor = 'val_acc', verbose = 0, save_best_only = True, mode = 'max' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step7 : Train the created <u>CNN</u> model\n",
    "The following script trains the created **<u>CNN</u>** with the provided checkpoint and created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harModel = myHarHandler.train_model( harModel, TrainX, TrainY, ValidationX, ValidationY, harModelCheckPoint )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Training accuracy', \n",
    "      round( 100 * accuracy_score( np.argmax( TrainY, axis = 1 ), \n",
    "                                  np.argmax( harModel.predict( TrainX ), axis = 1 ) ), 2 ) )\n",
    "print( 'Test accuracy', \n",
    "      round( 100 * accuracy_score( np.argmax( TestY, axis = 1 ), \n",
    "                                  np.argmax( harModel.predict( TestX ), axis = 1 ) ), 2 ) )\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score( np.argmax( ValidationY, axis = 1), \n",
    "                                  np.argmax( harModel.predict( ValidationX ), axis = 1 ) ), 2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step8: Validating the trained <u>CNN</u> model on test data\n",
    "The following section validates the trained network and creates a confusion matrix for the test dataset to have a detailed picture of the distributions of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myHarHandler.make_confusion_matrix(  harModel, TrainX, TrainY )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step9: Validating the trained <u>CNN</u> on the data acquire using High Speed Datalogger on STWIN\n",
    "Following we test the performance of the trained neural network on the data acquired using the High Speed Datalogger for each of the activities to see the how the model trained on the dataset performs on the data logged in live conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step9.1: Stationary activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = myDataHelper.hsdatalog_to_nn_segments( '../../Datasets/HSD_Logged_Data/HAR/Stationary/', \n",
    "                                                         activityName = 'Stationary' )\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score(np.argmax(labels, axis=1), \n",
    "                                  np.argmax(harModel.predict(segments), axis =1 )),2 ), '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step9.2: Walking activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = myDataHelper.hsdatalog_to_nn_segments( '../../Datasets/HSD_Logged_Data/HAR/Walking/', \n",
    "                                                         activityName = 'Walking' )\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score(np.argmax(labels, axis=1), \n",
    "                                  np.argmax(harModel.predict(segments), axis =1 )),2 ), '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step9.3: Jogging activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = myDataHelper.hsdatalog_to_nn_segments( '../../Datasets/HSD_Logged_Data/HAR/Jogging/', \n",
    "                                                         activityName = 'Jogging' )\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score(np.argmax(labels, axis=1), \n",
    "                                  np.argmax(harModel.predict(segments), axis =1 )),2 ), '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step9.4: Biking activity\n",
    "This section will only run when AST dataset is used which have `Biking` class. if WISDM dataset is used this section will generate an error as the `Biking` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = myDataHelper.hsdatalog_to_nn_segments( '../../Datasets/HSD_Logged_Data/HAR/Biking/', \n",
    "                                                         activityName = 'Biking' )\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score(np.argmax(labels, axis=1), \n",
    "                                  np.argmax(harModel.predict(segments), axis =1 )),2 ), '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step10: Create an npz file for validation after conversion from CubeAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataHelper.dump_data_for_post_validation( TestX, TestY, nrSamplesPostValid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional work to study the size of the dataset to obtain 85% performance\n",
    "This section is written to compare the performance of the **<u>CNN</u>** vs **<u>SVC</u>**.\n",
    "As the idea is that when small dataset is available we do not need the CNN for good performance instead SVC can be an easy approach to have comparable or even better performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.concatenate( ( TestX, ValidationX ) )\n",
    "testy = np.concatenate( ( TestY, ValidationY ) )\n",
    "print( 'samples,train_acc,test_acc')\n",
    "if( TrainY.shape[0] < 4000 ):\n",
    "    print( 'not enough samples present for this analysis' )\n",
    "else:\n",
    "    for ii in range( 100, 4000, 100 ):\n",
    "        harModelTest = myHarHandler.build_model()\n",
    "        harModelTest.fit( TrainX[ : ii ], TrainY[ : ii ], \n",
    "                 validation_split = 0.3, batch_size = 64, epochs = 30, verbose = 0 )\n",
    "        trainAcc = round( 100 * accuracy_score(np.argmax( TrainY, axis = 1 ), \n",
    "                                           np.argmax( harModelTest.predict( TrainX ), axis = 1 ) ), 2 ) \n",
    "        testAcc = round( 100 * accuracy_score( np.argmax( testy, axis = 1 ), \n",
    "                                          np.argmax( harModelTest.predict( testX ), axis = 1 ) ), 2 ) \n",
    "        print( '{},{},{}'.format( ii, trainAcc, testAcc ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "### code to generate the graph below\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./svm_vs_nn.txt', delimiter = ',', dtype = 'a' )\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.plot( data['samples'].values.astype(np.float32), data['svm_tt'].values.astype(np.float32), linewidth = 5 )\n",
    "plt.plot( data['samples'].values.astype(np.float32), data['nn_tt'].values.astype(np.float32), linewidth = 5 )\n",
    "plt.legend([ 'SVC', 'CNN' ], fontsize = 25)\n",
    "plt.xlabel('Training Samples', fontsize = 25)\n",
    "plt.ylabel('Accuracy [%]', fontsize = 25)\n",
    "plt.title('SVC vs CNN', fontsize = 35)\n",
    "plt.xticks(range(500, 4000, 500 ), fontsize = 25)\n",
    "plt.yticks(range(50, 110, 10 ), fontsize = 25)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig('svc_vs_cnn_acc.jpg')\n",
    "```\n",
    "<p align=\"center\">\n",
    "<img width=\"900\" height=\"400\" src=\"svc_vs_cnn_acc.jpg\">    \n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
