{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8 \n",
    "\n",
    "#   This software component is licensed by ST under BSD 3-Clause license,\n",
    "#   the \"License\"; You may not use this file except in compliance with the\n",
    "#   License. You may obtain a copy of the License at:\n",
    "#                        https://opensource.org/licenses/BSD-3-Clause\n",
    "  \n",
    "\n",
    "'''\n",
    "Training script of human activity recognition system (HAR), based on Support Vector Machine Classifier or (SVC).\n",
    "'''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step HAR using Classical Machine Learning and STM32CubeAI\n",
    "\n",
    "This notebook provides a step by step demonstration of a simple <u>H</u>uman <u>A</u>ctivity <u>R</u>ecognition (HAR) system, based on a <u>S</u>upport <u>V</u>ector Machine <u>C</u>lassifier (SVC). This script provides a simple data preperation script through `DataHelper` class and let user to preprocess, split, and segment the dataset to bring it into the form which can be used for training and validation of the Machine Learning model. \n",
    "\n",
    "All the implementations are done in Python and the machine learning part using [scikit-learn](https://scikit-learn.org/stable/getting_started.html) library.\n",
    "\n",
    "For demonstration purposes this script uses two datasets created for HAR using accelerometer sensor. \n",
    "\n",
    "* A public dataset provided by <u>WI</u>reless <u>S</u>ensing <u>D</u>ata <u>M</u>ining group named as **<u>WISDM</u>**. The details of the dataset are available [here](http://www.cis.fordham.edu/wisdm/dataset.php).\n",
    "\n",
    "* Our own propritery dataset called **<u>AST</u>**. \n",
    "\n",
    "**Note**: We are not providing any dataset in the function pack. The user can download WISDM dataset from [here](http://www.cis.fordham.edu/wisdm/dataset.php), while **<u>AST</u>** is a private dataset and is not provided. Although a subset of the **<u>AST</u>** dataset is provided in the function pack at location `/FP-AI-MONITOR1/Utilities/AI_Resources/Datasets/AST/`.\n",
    "\n",
    "Following figure shows the detailed workflow of HAR based on classical machine learning.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"760\" height=\"400\" src=\"workflow_ml.png\">    \n",
    "</p>\n",
    "\n",
    "Now, let us implement it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 : Import necessary dependencies\n",
    "Following section imports all the required dependencies. This also sets seeds for random number generator in Numpy to make the results more deterministic between different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import os, matplotlib.pyplot as plt, numpy as np, warnings\n",
    "from datetime import datetime\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# scikit-learn modules\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# private libraries\n",
    "from PrepareDataset import DataHelper\n",
    "import ML_Utilities\n",
    "from SVC_Utilities import *\n",
    "\n",
    "# disabling annoying warnings originating from python\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# setting the seeds to the random generators of Numpy and Tensorflow\n",
    "np.random.seed( 611 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Set environment variables\n",
    "Following section sets some user variables which will later be used for:\n",
    "* preparing the dataset for training and testing the machine learning model, and\n",
    "* post conversion validation of the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data variables\n",
    "dataset = 'AST'\n",
    "# dataset = 'WISDM'\n",
    "reducedClasses = True\n",
    "segmentLength = 24\n",
    "stepSize = 24\n",
    "preprocessing = True\n",
    "\n",
    "# training variables\n",
    "trainTestSplit = 0.6\n",
    "trainValidationSplit = 0.7\n",
    "nrSamplesPostValid = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Result directory\n",
    "Each run can have different variables depending on user settings and to compare the results of different choices, such as different segment size for the window for data, different overlap settings etc. We can save the results to compare different runs. \n",
    "\n",
    "Following section creates a result directory to save results for the current run. The name of the directory has following format. `Mmm_dd_yyyy_hh_mm_ss`, and example name for directory can be `Jul_20_2021_14_31_20/`. In every result directory there will a `.txt` file with information and an `onnx` model called `SVC_model.onnx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already exist create a parent directory for results.\n",
    "if not os.path.exists( './results_svm/'):\n",
    "    os.mkdir( './results_svm/' )\n",
    "svmResDir = 'results_svm/{}/'.format(datetime.now().strftime( \"%Y_%b_%d_%H_%M_%S\" ) )\n",
    "if not os.path.exists( svmResDir):\n",
    "    os.mkdir( svmResDir )\n",
    "infoString = 'runTime : {}\\nDatabase : {}\\nModel : {}\\nSeqLength : {}\\nStepSize : {}\\nReducedClasses : {}'.\\\n",
    "format( datetime.now().strftime(\"%Y-%b-%d at %H:%M:%S\"), dataset, 'SVC', segmentLength, stepSize, reducedClasses )\n",
    "with open( svmResDir + 'info.txt', 'w' ) as text_file:\n",
    "    text_file.write( infoString )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Create a `DataHelper` object\n",
    "The script in the following section creates a `DataHelper` object to preprocess, segment and split the dataset as well as to create the integer labels for the outputs to make the data training and testing ready using the choices set by the user in **Step2**.\n",
    "\n",
    "For example in case of **AST** dataset \n",
    "* the label `Stationary` will change to `0`.\n",
    "* the label `Walking` will change to `1`.\n",
    "* the label `Jogging` will change to `2`.\n",
    "* the label `Biking` will change to `3`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataHelper = DataHelper( dataset = dataset, reducedClasses = reducedClasses, \n",
    "                          seqLength = segmentLength, seqStep = stepSize, preprocessing = preprocessing,\n",
    "                          trainTestSplit = trainTestSplit, trainValidSplit = trainValidationSplit, resultDir = svmResDir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Prepare the dataset\n",
    "Following section prepares the dataset and create four tensors namely `trainX`, `trainy`, `testX`, `testy`. Each of the variables with trailing `X` are the inputs with shape `[_, ( segmentLength  * 3 ), 1 ]`and each of the variables with trailing `y` are corresponding outputs with shape `[ , _ ]`, where `_` correspond to the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy, testX, testy = myDataHelper.prepare_dataset_for_SVM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5.1: print the number of samples in train, test and validation data sets and the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Number of training samples : {}\\nNumber of test samples : {}\\nNumber of classes : {}'.\\\n",
    "      format( trainX.shape[ 0 ], testX.shape[ 0 ], len( myDataHelper.classes ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6: SVC\n",
    "```python\n",
    "# an example call to initialize an SVC object\n",
    "sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None\n",
    "```\n",
    "\n",
    "#### Step6.1: Run the grid search to find the optimized hyper parameters for SVC\n",
    "SVM based classifier object can be created with a set of parameters. Although a set of default parameter are provided the performance strongly depend on these parameters. The following code uses a function in `SVCUtilities.py` script to find the best parameters out of the grid. These parameters are choosen to provide the best peformance. Following section computes and prints these values.\n",
    "\n",
    "**Note**: The search of these parameters can take long time, we have provided precomputed values for the sake of running scripts quickly. Setting `precomputed = True` value will result in return of these precomputed values. Setting this to `False` will result in a computation of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestParams = ParamSearch( trainX, trainy, precomputed = True ) # set precomputed to false if you want to perform grid search\n",
    "print( bestParams )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6.2: Creating an SVC model with just 2% of the training data and seeing the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step6.2.1: initializing the SVC\n",
    "\n",
    "The following script initializes the an object of SVC using the best parameters computed from the section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC( kernel = bestParams[ 'kernel' ], C = bestParams[ 'C' ], gamma = bestParams[ 'gamma' ], \n",
    "          decision_function_shape='ovo', probability = True ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step6.2.2: fitting the svc_model to the 2% of the data\n",
    "Unlike a training in Neural networks which required multiple iterations over the data, when performing a training of the traditional machine learning model a fit function is enough to call.\n",
    "\n",
    "One of the power of the traditional machine learning algorithm is that they can be very performant when a small dataset is available to train. To demonstrate that, the following section uses only the `2%` of the training dataset for the training purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetInd = max( int( len( trainX ) / 50 ), 100 ) # if number of samples are lesser than 5000 use 100 samples\n",
    "svc_model.fit( trainX[ : subsetInd ], trainy[ : subsetInd ] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step6.2.3: predicting the accuracy of the svc model\n",
    "Once the model is fit the next step is to test it and see the accuracy of the model on the `full` train dataset and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( 'train accuracy', np.round( 100 * accuracy_score( trainy, svc_model.predict( trainX ) ), 2 ) )\n",
    "print( 'test accuracy', np.round( 100 * accuracy_score( testy, svc_model.predict( testX ) ), 2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step6.2.4: Saving the model\n",
    "The model can be saved as onnx model now using the `ML_Utilities.py` script. More specifically `model_conversion_to_onnx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Utilities.model_conversion_to_onnx( svc_model, trainX, svmResDir + 'svc_2_pct.onnx', svmResDir + 'svc_2_pct.json' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step7: Dimensionality reduction of the data before machine learning model\n",
    "Instead of using the raw data itself, we can be smarter and use the dimensionality reduction based on PCA or SVD.\n",
    "This dimensionality reduction has two fold benefits, which are:\n",
    "* improved accuracy by removing the unrelevant information,\n",
    "* reduced sizes of the model by having fewer data to save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7.1: building a pipeline for the machine learning model along with the dimensional reduction\n",
    "Thanks to `scikit-learn` pipeline functionality it is quite easy to implement multiple signal processing functions in a single object. Following we are building a pipeline object which implements `Dimensionality_reduction` and `SVC` at once like a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use 24 samples instead of all 72\n",
    "dim_reduction = TruncatedSVD( n_components = 24, algorithm = 'arpack', random_state = None )\n",
    "svc_model = Pipeline( [ ( 'dim_reduction', dim_reduction ),\n",
    "                       ('svc', SVC( kernel = bestParams['kernel'], C = bestParams['C'], gamma = bestParams['gamma'], \n",
    "          decision_function_shape='ovo', probability = True ) ) ] )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7.1.2: Training the pipeline model\n",
    "To show the benefits of the dimensionality reduction we are going to traing the new model with the very same data and see the increase in the performance and reduction in the model size.\n",
    "\n",
    "Following section trains the model with dimensionality reduction and with just `2%` of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model.fit( trainX[ : subsetInd ], trainy[ : subsetInd ] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7.1.3: Testing the pipeline model\n",
    "Following section tests the accuracy of the trained pipeline model on the `full` training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'train accuracy', np.round( 100 * accuracy_score( trainy, svc_model.predict( trainX ) ), 2 ) )\n",
    "print( 'test accuracy', np.round( 100 * accuracy_score( testy, svc_model.predict( testX ) ), 2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step7.1.4: Testing the pipeline model\n",
    "Following section saves the trained model. One can compare the performance and the size of the models to see the benefits of the dimensionality reduction before applying machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Utilities.model_conversion_to_onnx( svc_model, trainX, svmResDir + 'svc_2_pct_dim_red.onnx', svmResDir + 'svc_2_pct_dim_red.json' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step8: Validating the created model on the test data\n",
    "Following we plot the confusion matrix of the prediction of the model on the test data to understand the distribution of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cM = confusion_matrix( testy, svc_model.predict( testX ) )\n",
    "cmNormalized = np.round( 100 * cM / np.sum( cM, axis = 1 ), 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating the confusion matrix figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Confusion matrix \\n Nr. of Occurences and \\n %age confidence'\n",
    "# creating a figure object\n",
    "fig, ax = plt.subplots( figsize = ( 7, 7 ) )\n",
    "# plotting the confusion matrix\n",
    "im = ax.imshow( cmNormalized, interpolation = 'nearest', cmap = plt.cm.Blues )\n",
    "\n",
    "# assiging the title, x and y labels\n",
    "plt.xlabel( 'Predicted Values' )\n",
    "plt.ylabel( 'Ground Truth' )\n",
    "plt.title( title )\n",
    "\n",
    "# defining the ticks for the x and y axis\n",
    "plt.xticks( range( len( myDataHelper.classes ) ), myDataHelper.classes, rotation = 60 )\n",
    "plt.yticks( range( len( myDataHelper.classes ) ), myDataHelper.classes )\n",
    "plt.ylim( bottom = len( myDataHelper.classes ) - .5 )\n",
    "plt.ylim( top = -0.5 )\n",
    "\n",
    "# annotating the confusion matrix with values and printing in shall the accuracy for each class\n",
    "width, height = cM.shape \n",
    "\n",
    "print( 'Accuracy for each class is given below.' )\n",
    "for predicted in range( width ):\n",
    "    for real in range( height ):\n",
    "        if cmNormalized[ predicted,real ] > 45:\n",
    "            color = 'white'\n",
    "        else:\n",
    "            # background will be two light to have a white colored text\n",
    "            color = 'black'\n",
    "        \n",
    "        if( predicted == real ):\n",
    "            print( myDataHelper.classes[ predicted ].ljust( 12 )+ ':', cmNormalized[ predicted,real ], '%' )\n",
    "        plt.gca( ).annotate( '{}\\n{}%'.format( cM[ predicted, real ], cmNormalized[ predicted, real ] ), xy = ( real, predicted ),\n",
    "            horizontalalignment = 'center', verticalalignment = 'center', color = color )\n",
    "\n",
    "# showing colorbar for normalized confusion matrix case\n",
    "# creating a color bar and setting the limits\n",
    "divider = make_axes_locatable( plt.gca( ) )\n",
    "cax = divider.append_axes( \"right\", \"5%\", pad=\"3%\" )\n",
    "plt.colorbar( im, cax = cax )\n",
    "\n",
    "# making sure that the figure is not clipped\n",
    "plt.tight_layout( )\n",
    "\n",
    "# showing plot\n",
    "plt.savefig( '{}svc_2_pct_dim_red_confusion_matrix.png'.format( svmResDir )  )\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step9: Validating <u>SVC</u> model created on dataset using data logged using High Speed Datalogger on STWIN\n",
    "Following script generates segments from a High Speed Datalogger binary provided in `/FP-AI-MONITOR1/Utilities/Datalog/` and evaluates the performance of the trained model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationary activitiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = myDataHelper.hsdatalog_to_ml_segments( '../../Datasets/HSD_Logged_Data/HAR/Stationary/', \n",
    "                                                         activityName = 'Stationary' )\n",
    "\n",
    "\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score( labels, svc_model.predict( segments ) ), 2 ), '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walking activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = myDataHelper.hsdatalog_to_ml_segments( '../../Datasets/HSD_Logged_Data/HAR/Walking/', \n",
    "                                                         activityName = 'Walking' )\n",
    "\n",
    "\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score( labels, svc_model.predict( segments ) ), 2 ), '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jogging activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = myDataHelper.hsdatalog_to_ml_segments( '../../Datasets/HSD_Logged_Data/HAR/Jogging/', \n",
    "                                                         activityName = 'Jogging' )\n",
    "\n",
    "\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score( labels, svc_model.predict( segments ) ), 2 ), '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biking activity\n",
    "\n",
    "This section will only run when AST dataset is used which have `Biking` class. if WISDM dataset is used this section will generate an error as the `Biking` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = myDataHelper.hsdatalog_to_ml_segments( '../../Datasets/HSD_Logged_Data/HAR/Biking/', \n",
    "                                                         activityName = 'Biking' )\n",
    "\n",
    "\n",
    "print( 'Validation accuracy', \n",
    "      round( 100 * accuracy_score( labels, svc_model.predict( segments ) ), 2 ), '%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step10: Create an npz file for validation after conversion from CubeAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataHelper.dump_data_for_post_validation( testX, testy, nrSamplesPostValid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional work to study the size of the dataset to obtain 85% performance\n",
    "This section is written to compare the performance of the **<u>CNN</u>** vs **<u>SVC</u>**.\n",
    "As the idea is that when small dataset is available we do not need the CNN for good performance instead SVC can be an easy approach to have comparable or even better performances.\n",
    "\n",
    "##### Note: This section only works with the full dataset, i.e. when there are enough samples available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir( svmResDir ):\n",
    "    os.mkdir(svmResDir)\n",
    "print('samples,train_acc,test_acc')\n",
    "dim_reduction = TruncatedSVD( n_components = 24, algorithm = 'arpack', random_state = None )\n",
    "model = Pipeline( [ ( 'dim_reduction', dim_reduction ),\n",
    "                   ( 'svc', SVC( kernel = bestParams['kernel'], C = bestParams['C'], gamma = bestParams['gamma'],\n",
    "                    decision_function_shape='ovo', probability = True ) ) ] )\n",
    "if( trainX.shape[0] > 4000 ):\n",
    "    for ii in range( 100, 4000, 100 ):\n",
    "        model.fit( trainX[:ii], trainy[:ii] )\n",
    "        train_acc =  np.round( 100 * accuracy_score( trainy, model.predict( trainX ) ), 2 )\n",
    "        test_acc =  np.round( 100 * accuracy_score( testy, model.predict( testX ) ), 2 )\n",
    "        print( '{},{},{}'.format( ii, train_acc, test_acc ) )\n",
    "else:\n",
    "    print('Not enough samples available for this study')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "### code to generate the graph below\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./svm_vs_nn.txt', delimiter = ',', dtype = 'a' )\n",
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.plot( data['samples'].values.astype(np.float32), data['svm_tt'].values.astype(np.float32), linewidth = 5 )\n",
    "plt.plot( data['samples'].values.astype(np.float32), data['nn_tt'].values.astype(np.float32), linewidth = 5 )\n",
    "plt.legend([ 'SVC', 'CNN' ], fontsize = 25)\n",
    "plt.xlabel('Training Samples', fontsize = 25)\n",
    "plt.ylabel('Accuracy [%]', fontsize = 25)\n",
    "plt.title('SVC vs CNN', fontsize = 35)\n",
    "plt.xticks(range(500, 4000, 500 ), fontsize = 25)\n",
    "plt.yticks(range(50, 110, 10 ), fontsize = 25)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig('svc_vs_cnn_acc.jpg')\n",
    "```\n",
    "<p align=\"center\">\n",
    "<img width=\"900\" height=\"400\" src=\"svc_vs_cnn_acc.jpg\">    \n",
    "</p>\n",
    "<b>Note : </b> These graphs are generated using the full AST dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
