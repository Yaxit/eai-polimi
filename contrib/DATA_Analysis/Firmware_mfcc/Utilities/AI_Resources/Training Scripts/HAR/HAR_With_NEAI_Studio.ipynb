{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8 \n",
    "\n",
    "#   This software component is licensed by ST under BSD 3-Clause license,\n",
    "#   the \"License\"; You may not use this file except in compliance with the\n",
    "#   License. You may obtain a copy of the License at:\n",
    "#                        https://opensource.org/licenses/BSD-3-Clause\n",
    "  \n",
    "\n",
    "# Script to prepare the data for the Human Activitiy Recognition using Cartesiam NanoEdge AI Studio.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step HAR using Classical Machine Learning and STM32CubeAI\n",
    "\n",
    "This notebook provides a step by step demonstration of a simple script to prepare input segments for NanoEdge&trade; AI Studio for <u>H</u>uman <u>A</u>ctivity <u>R</u>ecognition (HAR) database. This script relies on a simple data preperation script through `DataHelper` class (used to prepare data for CNN and SVM also) and let user to preprocess, and segment the dataset to bring it into the form which can be used for generating the classification libraries with NanoEdge&trade; AI Studio.\n",
    "\n",
    "For demonstration purposes this script uses two datasets created for HAR using accelerometer sensor. \n",
    "\n",
    "* A public dataset provided by <u>WI</u>reless <u>S</u>ensing <u>D</u>ata <u>M</u>ining group named as **<u>WISDM</u>**. The details of the dataset are available [here](http://www.cis.fordham.edu/wisdm/dataset.php).\n",
    "\n",
    "* Our own propritery dataset called **<u>AST</u>**. \n",
    "\n",
    "**Note**: We are not providing any dataset in the function pack. The user can download WISDM dataset from [here](http://www.cis.fordham.edu/wisdm/dataset.php), while **<u>AST</u>** is a private dataset and is not provided. Although a subset of the **<u>AST</u>** dataset is provided in the function pack at location `/FP-AI-MONITOR1/Utilities/AI_Resources/Datasets/AST/`.\n",
    "\n",
    "Following figure shows the detailed workflow of HAR based on NanoEdge&trade; AI library.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"760\" height=\"400\" src=\"workflow_neai.png\">    \n",
    "</p>\n",
    "\n",
    "Now, let us implement it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 : Import necessary dependencies\n",
    "Following section imports all the required dependencies. This also sets seeds for random number generator in Numpy to make the results more deterministic between different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# python libraries\n",
    "import os, numpy as np, warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# private libraries\n",
    "from PrepareDataset import DataHelper\n",
    "\n",
    "# disabling annoying warnings originating from python\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# setting the seeds to the random generators of Numpy and Tensorflow\n",
    "np.random.seed( 611 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Set environment variables\n",
    "Following section sets some user variables which will later be used for preparing the dataset for training and creating the NanoEdge AI libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data variables\n",
    "dataset = 'AST'\n",
    "# dataset = 'WISDM'\n",
    "reducedClasses = True\n",
    "segmentLength = 24\n",
    "stepSize = 24\n",
    "preprocessing = True\n",
    "\n",
    "# training variables (to split data into train, valid and test sets)\n",
    "trainTestSplit = 0.6\n",
    "trainValidationSplit = 0.7\n",
    "nrSamplesPerClass = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Result directory\n",
    "Saving the created data segment files in the data_neai folder along with the configurations used to create this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not already exist create a parent directory for results.\n",
    "resDir = './data_neai/'\n",
    "if not os.path.exists( resDir ):\n",
    "    os.mkdir( resDir )\n",
    "infoString = 'runTime : {}\\nDatabase : {}\\nModel : {}\\nSeqLength : {}\\nStepSize : {}\\nReducedClasses : {}\\nnrSamplesPerClass : {}'.\\\n",
    "format( datetime.now().strftime(\"%Y-%b-%d at %H:%M:%S\"), dataset, 'SVC', segmentLength, stepSize, reducedClasses, nrSamplesPerClass )\n",
    "with open( resDir + 'info.txt', 'w' ) as text_file:\n",
    "    text_file.write( infoString )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Create a `DataHelper` object\n",
    "The script in the following section creates a `DataHelper` object to preprocess, segment and split the dataset as well as to create the labels for the outputs to make the data training and testing ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataHelper = DataHelper( dataset = dataset, reducedClasses = reducedClasses, \n",
    "                          seqLength = segmentLength, seqStep = stepSize, preprocessing = preprocessing,\n",
    "                          trainTestSplit = trainTestSplit, trainValidSplit = trainValidationSplit, resultDir = resDir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Prepare the dataset\n",
    "Following section prepares the dataset and create four tensors namely `trainX`, `trainy`, `testX`, `testy`. Each of the variables with trailing `X` are the inputs with shape `[_, ( segmentLength  * 3 ), 1 ]`and each of the variables with trailing `y` are corresponding outputs with shape `[ , _ ]`, where `_` correspond to the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy, _, _, _, _ = myDataHelper.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5.1: print the number of samples in train, test and validation data sets and the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Number of training samples : {}\\nNumber of test samples : {}\\nNumber of classes : {}\\nClass labels : {}'.\\\n",
    "      format( trainX.shape, trainy.shape, len( myDataHelper.classes ), myDataHelper.classes ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: reshaping the data to comply with NanoEdge&trade AI Studio\n",
    "NanoEdge AI Studio expects the data for multiple axes to be vectorized using the followini convention\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x_1 & y_1 & z_1\\\\\n",
    "x_2 & y_2 & z_2 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "x_n & y_n & z_n\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\end{pmatrix}\n",
    " => \n",
    "\\begin{pmatrix}\n",
    "x_1 & y_1 & z_1 & x_2 & y_2 & z_2 & \\dots & \\dots & \\dots & x_{24} & y_{24} & z_{24} \\\\\n",
    "x_{25} & y_{25} & z_{25} & x_{26} & y_{26} & z_{26} & \\ddots & \\ddots & \\ddots & x_{48} & y_{48} & z_{48} \\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Following section vectorize randomly choses `nrSamplesPerClass` samples for each class, vectorize them and dump them in a csv file with the title Class label.csv for example `Stationary.csv` in the resDir folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataHelper.create_neai_segments( trainX, trainy, nrSamplesPerClass )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
